---
layout: blog_post
title: An Enterprise Data Capability Roadmap
summary: Today's enterprise data platforms are required to support agile and predictive workflows.
img: /img/blank-road-sign.png
date: April 28, 2013
author: Mark Moloney
omit_img: false
omit_title: false
---

Today's enterprise data platforms are required to support agile and predictive workflows. Data warehouses and OLAP
are mature but usually oriented to the analysis of historical data. The capability to
analyze varied and often messy sets of data with a schema that evolves as the analysis progresses hasn't been
supported well. And for various reasons, data capabilities have been fragmented:

* Business intelligence and data warehousing owned by IT
* Statistical analysis that may exist within multiple functions, e.g. marketing and credit risk
* Data services and products

There is a great deal of value in the existing systems and processes. For example, if the numbers in a financial report
don't add up to match say the monthly total because some transactions failed to load or because the selected
dimensions of the report are not conformed, then the report is not trusted and decisions are made using the old
failsafe of gut feel. In a more connected and competitive market, gut feel doesn't cut it.

The image below is a map of the emerging enterprise data capability. The picture is not perfect and nor are the
technologies, but it is an approach to begin to merge capabilities and handle growing data volumes, with the
opportunities and risks which that growth entails.

<br>
![Enterprise Data Platform](/img/data_platform.jpg "Enterprise Data Platform")
<br>

The Metadata Registry is a key component of this emerging architecture to address the
<a href="http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf"
target="_blank">three of more V's of data</a>
and the multi-vendor environment assuming a one-size-fits-all solution will not deliver the agility and best-in-class
support for each type of workload and content. For example, Hadoop-based platforms are enabling data that previously
would have been discarded to be retained and made available for analysis. The ideal solution is to deliver new
capabilities while reusing existing information assets to improve the overall quality of results.

This architecture provides an agile facility for domain specialists to perform discovery and
run the kinds of jobs that would have been outlawed on previous infrastructure. And when the results of useful
analyses are required on a recurring basis, the process is productionized
with the appropriate governance and quality controls. The Metadata Registry keeps track of data across these
states of management, helping to document and contextualize the data, ensuring that compliance
obligations such as security are being met, and supporting the migration of data from one form and state of
management to another.

Finally, a key dependency is the right culture that values <a href="http://prezi.com/olytaplf65pq/building-multi-disciplinary-analytics-teams/"
target="_blank">multi-disciplinary teams</a> open to new ideas and diversity in
skill set and experience. For example, software development as a discipline has learned to value agile practices
such as team empowerment, test-first development, and small incremental releases. People with backgrounds in
experimental science have much to teach about experimental design and error analysis. The best ideas are often
found at the intersection of disciplines.

->![Multidisciplinary Team](/img/data_roles.jpg "Multidisciplinary Team")<-
